[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FAQ about Propensity Score",
    "section": "",
    "text": "Preface\nThis is a FAQ for the propensity score matching lecture.\n\nMain reference\n\n\n\n\n\n\nReference\n\n\n\nKarim, ME (2021) “Understanding Propensity Score Matching”, URL: ehsanx.github.io/psw/\n\n\n\n\nVersion history\nMaterials were prepared for the deliveries of the following lecture:\n\n\n\n\n\n\nLecture\n\n\n\n\n‘Introduction to Causal inference’\n\nMarch 20, 2023, prepared for Guest Lecture in SPPH 500/007 (Analytical Methods in Epidemiological Research) at UBC\n\n\n\n\n\n\nComments\nFor any comments regarding this document, reach out to me."
  },
  {
    "objectID": "goal.html#prediction-goal",
    "href": "goal.html#prediction-goal",
    "title": "1  Epideliologic Research Goals",
    "section": "1.1 Prediction goal",
    "text": "1.1 Prediction goal\nThe primary objective of a prediction goal is to forecast the occurrence or risk of an outcome (Y) based on one or more risk factors (A, L). The focus of this goal is often on making accurate predictions using statistical and machine learning techniques to identify patterns and relationships in the data. Prediction goals help to identify populations at risk and inform targeted prevention strategies.\n\n\n\n\n\n\nTotal cholesterol level example\n\n\n\nA predictive goal, on the other hand, would involve developing a model that can accurately predict total cholesterol levels based on various factors, including Rosuvastatin use and other relevant predictors (e.g., race, sex, and age).\n\n\n\n\n\n\nflowchart LR\n  A[Rosuvastatin] --> Y(Total cholesterol level)\n  B[Race] --> Y\n  C[Sex] --> Y\n  D[Age] --> Y"
  },
  {
    "objectID": "goal.html#causal-goal",
    "href": "goal.html#causal-goal",
    "title": "1  Epideliologic Research Goals",
    "section": "1.2 Causal goal",
    "text": "1.2 Causal goal\nThe causal goal focuses on understanding the causal relationship between a risk factor (often a treatment, A) and a health outcome (Y). It aims to identify the factors that contribute to the development, progression, or prevention of a specific disease or health outcome. Control for confounding factors (L) is often a necessary step in understanding such a relationship, as these factors may obscure the true causal relationship between the treatment and the outcome. The focus of this goal is often on estimating the parameter ‘treatment effect’, which represents the causal effect of the treatment (A) on the outcome (Y). Causal goals guide the development of effective interventions and policies by understanding the mechanism by which the factors influence health outcomes.\n\n\n\n\n\n\nTotal cholesterol level example\n\n\n\nCausal goal helps understanding the causal effect of Rosuvastatin on total cholesterol levels while accounting for confounding factors such as race, sex, and age.\n\n\n\n\n\n\nflowchart LR\n  A[Rosuvastatin] --> Y(Total cholesterol level)\n  B[Race] --> Y\n  C[Sex] --> Y\n  D[Age] --> Y\n  B --> A\n  C --> A\n  D --> A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere can be a overlaps between causal and predictive goals in epidemiological research, but they serve distinct purposes and have different focuses.\n\nUnderstanding the causal relationships between variables can help improve the accuracy of predictive models. For example, if you know that a certain factor causally affects a health outcome, incorporating that factor into the prediction model can lead to better predictions.\nThe process of building a predictive model may shed light on potential causal relationships between variables that could be further investigated in causal analyses."
  },
  {
    "objectID": "steps.html#step-1-propensity-score-estimation",
    "href": "steps.html#step-1-propensity-score-estimation",
    "title": "2  Steps of Propensity score Matching",
    "section": "2.1 Step 1: Propensity score estimation",
    "text": "2.1 Step 1: Propensity score estimation\n\n\n\n\n\nflowchart LR\nsubgraph ZA[\" \"]\ndirection LR\n  A[(Exposure <br/>modelling <br/>to<br/> estimate <br/>propensity <br/>scores)]  --> A1(Variable selection)\n  A --> A2(Model specification)\nend\n\nsubgraph ZAA[\" \"]\ndirection LR\n  A1 --> A11[Consult  <br/>subject-area  <br/>experts]\n  A1 --> A13[Build DAG  <br/>from  <br/>literature review]\n  A1 --> A12[Analytic methods  <br/>to sort out  <br/>problematic variables-  <br/>collinearity]\n  A2 --> A21[Add interactions]\n  A2 --> A22[Add polynomial terms]\n  A2 --> A23[Add complex functions  <br/>of one or more  <br/>covariates]\n  A2 --> A24[Use machine learning  <br/>methods to identify  <br/>complex patterns]\nend\n\nsubgraph ZAAA[\" \"]\ndirection LR\n  A21 --> A221[Unstable <br/>results - <br/>exposure <br/>model <br/>coef SEs <br/>are too <br/>large <br/>or infinite-<br/> Go back to <br/>Model <br/>specification <br/>step]\n  A22 --> A221\n  A23 --> A221\n  A24 --> A221\n  A11 --> A111[Carefully think <br/>about <br/>variable roles]\n  A13 --> A111\n  A12 --> A112[Merge covariates, <br/>covariate levels, <br/>omit some <br/>variables, <br/>find alternative and <br/>less problematic <br/>variable]\n  style A111 fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5\nend\n\nZA -.-> ZAA -.-> ZAAA\n\n\n\n\n\n\n\n\nThe type of variables we want to include in the propensity score modelling: this should be based on prior knowledge. Usually empirical selection of variables is discouraged. Understanding the role of each variable, e.g., effect modifier vs. confounder will determine the analysis strategy.\n\n\n\n\nflowchart LR\n  A1110[\\Variable roles\\] --> A1111{{Select variables <br/>causing outcome}}\n  A1110 --> A1112{{select variables <br/>causing both outcome <br/>and exposure}}\n  A1110 --> A1113{{omit variables <br/>only causing <br/>exposure}}\n  A1110 --> A1114{{omit variables <br/>in the causal <br/>pathway}}\n  A1110 --> A1115{{omit variables <br/>that are effect <br/>of outcome}}\n  A1110 --> A1116{{omit variables <br/>that are <br/>simply noise}}\n  A1110 --> A1117{{add useful <br/>proxies}}\n  style A1110 fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5"
  },
  {
    "objectID": "steps.html#step-2-propensity-score-matching",
    "href": "steps.html#step-2-propensity-score-matching",
    "title": "2  Steps of Propensity score Matching",
    "section": "2.2 Step 2: Propensity score matching",
    "text": "2.2 Step 2: Propensity score matching\n\n\n\n\nflowchart LR\nsubgraph ZA[\" \"]\ndirection LR\n  B[(Propensity <br/>score <br/>Matching)] --> B1(Matching<br/> methods)\n  B --> B2(Matching <br/>ratios)\n  B --> B3(replacements)\nend\n\nsubgraph ZAA[\" \"]\ndirection LR\n  B1 --> B11[Nearest <br/>neighbor]\n  B1 --> B12[Caliper, <br/>0.2*SD of <br/>logit of <br/>propensity <br/>score]\n  B1 --> B13[Optimal]\n  B1 --> B14[Full]\n  B2 --> B21[Fixed <br/>ratio]\n  B2 --> B23[Variable <br/>ratio]\n  B3 --> B33[With]\n  B3 --> B34[Without]\nend\n\nsubgraph ZAAA[\" \"]\ndirection RL\n  B21 --> B211[1:1, pair]\n  B21 --> B212[1:M,<br/> where M>1 <br/>can be <br/>any integer]\n  B1 --> B0>Some <br/>combinations <br/>assuming <br/>sample size <br/>do not <br/>reduce <br/>too much]\n  B2 --> B0\n  B3 --> B0\nend\n\nZA -.-> ZAA -.-> ZAAA"
  },
  {
    "objectID": "steps.html#step-3-balance-of-propensity-score-matched-dataset",
    "href": "steps.html#step-3-balance-of-propensity-score-matched-dataset",
    "title": "2  Steps of Propensity score Matching",
    "section": "2.3 Step 3: Balance of Propensity score matched dataset",
    "text": "2.3 Step 3: Balance of Propensity score matched dataset\n\n\n\n\n\nflowchart TD\nsubgraph ZA[\" \"]\ndirection LR\n  C[(Assess balance <br/>and overlap)] --> C1(SMD)\n  C --> C2(Variance  <br/>ratio)\n  C --> C3(Visualization, <br/>overlapping histograms <br/>love plot, <br/>balance tables)\nend\n\nsubgraph ZAA[\" \"]\ndirection LR\n  C1 --> C11[\\Unsatisfactory balance  <br/>or overlap\\]\n  C2 --> C11\n  C3 --> C11\n  C3 --> C31[\\Propensity scores <br/>too close <br/>to 0 or 1\\]\nend\n\nsubgraph ZAAA[\" \"]\ndirection RL\n  C11 --> C111[/Go back to <br/>Exposure modelling step\\]\n  C31 --> C111\n  C31 --> C112[/Trimming, <br/>not preferred\\]\nend\n\nZA -.-> ZAA -.-> ZAAA"
  },
  {
    "objectID": "steps.html#step-4-treatment-effect-estimation-from-outcome-model",
    "href": "steps.html#step-4-treatment-effect-estimation-from-outcome-model",
    "title": "2  Steps of Propensity score Matching",
    "section": "2.4 Step 4: treatment effect estimation from outcome model",
    "text": "2.4 Step 4: treatment effect estimation from outcome model\n\n\n\n\nflowchart TD\nsubgraph ZA[\" \"]\ndirection LR\n  D[(Outcome <br/>modelling)] --> D1(Crude)\n  D --> D2(Adjusted)\n  D --> D4(Variance <br/>estimation)\nend\n\nsubgraph ZAA[\" \"]\ndirection LR\n  D2 --> D221[All covariates, <br/>preferred]\n  D2 --> D222[Partial list <br/>of covariates]\n  D4 --> D41[Cluster options]\n  D4 --> D42[Bootstrap options]\nend\n\nZA -.-> ZAA"
  },
  {
    "objectID": "steps.html#reporting",
    "href": "steps.html#reporting",
    "title": "2  Steps of Propensity score Matching",
    "section": "2.5 Reporting",
    "text": "2.5 Reporting\n\nWe should report the results and interpret the treatment effect estimates in the context of the research question and the underlying assumptions.\nWe need to clearly communicate the limitations and potential biases in the analysis, and reporting of results from useful sensitivity analyses.\n\n\n\n\n\nflowchart TD\nsubgraph ZA[\" \"]\ndirection LR\n  D6(Sensitivity <br/>Analysis <br/>for <br/>overall process) --> D5(Sensitivity <br/>Analysis <br/>for unmeasured <br/>confounding)\nend\n\nsubgraph ZAA[\" \"]\ndirection LR\n  D5 --> D51[Rosenbaum <br/>bounds]\n  D5 --> D52[Quantitative <br/>bias analysis, <br/>E-value]\n  D6 --> D62[Alternative <br/>matching <br/>algorithm]\n  D6 --> D61[Alternative <br/>model <br/>specifications]\n  D6 --> D63[Alternative <br/>missing data <br/>method]\nend\n\nZA -.-> ZAA\n\n\n\n\n\n\n\n\n\nWe should also discuss the implications of the findings for the target population and the broader scientific literature."
  },
  {
    "objectID": "steps.html#uses-of-propensity-score-matching",
    "href": "steps.html#uses-of-propensity-score-matching",
    "title": "2  Steps of Propensity score Matching",
    "section": "2.6 Uses of propensity score matching",
    "text": "2.6 Uses of propensity score matching\nOther than answering causal question in observational studies, propensity score matching can be helpful in the following scenarios:\nCovariate adjustment in randomized trials: In some cases, propensity score matching can be applied to randomized controlled trials to adjust for potential imbalances in covariates that may arise due to randomization, especially in small sample sizes. However, there is a lot of controversy regarding covariate adjustment in RCTs.\nSubgroup analysis: Propensity score matching can be used to conduct subgroup analysis, where the goal is to explore treatment effects within specific subgroups defined by one or more covariates."
  },
  {
    "objectID": "assumptions.html#conditional-exchangeability",
    "href": "assumptions.html#conditional-exchangeability",
    "title": "3  Identifiability Conditions",
    "section": "3.1 Conditional exchangeability",
    "text": "3.1 Conditional exchangeability\n\n\n\n\n\n\nTotal cholesterol level example\n\n\n\nAssume a condition where doctors prescribe Rosuvastatin more often if the patient belongs to any of the following category\n\nWhite (race)\nMale (sex)\nAge (Age at least 50)\n\n\n\nConditional exchangeability, also known as conditional ignorability or unconfoundedness, is an assumption that treatment assignment is independent of potential outcomes, given a set of measured confounders. If conditional exchangeability is satisfied, we can consider the treatment and control groups exchangeable or similar in terms of potential outcomes.\n\n\n\\(Y(A=1), Y(A=0) \\perp A | L\\): Treatment assignment is independent of the potential outcome, given L\n\n\n\n\n\n\nTotal cholesterol level example\n\n\n\nTo achieve conditional exchangeability, we need to balance the distribution of these confounders (race, sex, and age) between the treatment and control groups.\n\n\nBy doing so, we can minimize the impact of confounding variables on the treatment-outcome relationship, allowing for a more accurate estimation of the causal effect of Rosuvastatin intake on total cholesterol levels.\n\n\n\n\n\n\nReal-world Data Analysis\n\n\n\nAchieving conditional exchangeability can be challenging, especially in observational studies where treatment assignment is not randomized. General suggestions in a real-world scenario:\n\nTo strengthen the plausibility of the conditional exchangeability assumption, researchers should carefully identify and measure all relevant confounders that might influence both the treatment assignment and the outcome. This often requires domain-specific knowledge and a thorough understanding of the causal relationships among variables.\nIt is impossible to prove that conditional exchangeability holds in a given study. Therefore, researchers should perform sensitivity analyses to assess the robustness of their causal effect estimates to potential violations of this assumption.\nResearchers should be transparent about the assumptions made in their analyses, including conditional exchangeability, and clearly communicate the limitations of their study findings due to these assumptions."
  },
  {
    "objectID": "assumptions.html#positivity",
    "href": "assumptions.html#positivity",
    "title": "3  Identifiability Conditions",
    "section": "3.2 Positivity",
    "text": "3.2 Positivity\nPositivity is an important assumption in the causal inference that ensures a non-zero probability of receiving each treatment level (or exposure) for every stratum of the confounding variables. In other words, it means that for every combination of the confounders, there should be at least some individuals who have received and not received the treatment.\n\n\n\\(0 < P(A=1 | L) < 1\\): Subjects are eligible to receive both treatment, given L\n\n\n\n\n\n\nTotal cholesterol level example\n\n\n\nIn the context of the association between Rosuvastatin intake and total cholesterol levels, where race, sex, and age are confounders, the positivity assumption implies that there must be people from every combination of race, sex, and age who have taken Rosuvastatin as well as those who haven’t.\nAssume that there are 2 race categories, 2 sex categories, and 2 age categories in the data. Then we will have a total of \\(2 \\times 2 \\times 2\\) = 8 possible categories. The following 8 categories represent all possible combinations of the given race, sex, and age categories:\n\nWhite, Male, Less than 50\nWhite, Male, At least 50\nWhite, Female, Less than 50\nWhite, Female, At least 50\nAsian, Male, Less than 50\nAsian, Male, At least 50\nAsian, Female, Less than 50\nAsian, Female, At least 50\n\nThe positivity assumption implies that there should be individuals taking (\\(A=1\\)) and not taking the treatment (\\(A=0\\)) in each of these 8 categories.\n\n\n\n\n\n\n\n\nSolutions and Consequences: no free lunch!\n\n\n\n\nIf there are no control individuals (not receiving the treatment) for the “Asian, Male, At least 50” category, it means the positivity assumption is violated for this particular combination of confounders.\nIf the sample size allows, we could consider combining categories, such as merging age groups or collapsing race categories, to create more balanced groups. This may help to satisfy the positivity assumption but might also result in a loss of information or reduced ability to detect specific treatment effects.\nThe causal effect estimates obtained from the study may not be generalizable to the entire population, as the results might not be applicable to the “Asian, Male, At least 50” category. This limits the external validity of the study findings."
  },
  {
    "objectID": "assumptions.html#causal-consistency",
    "href": "assumptions.html#causal-consistency",
    "title": "3  Identifiability Conditions",
    "section": "3.3 (Causal) consistency",
    "text": "3.3 (Causal) consistency\nWhen the causal consistency assumption holds, it implies that the treatment is well-defined, and there is only one version of the treatment that can be applied to all treated individuals. This allows for a clear interpretation of the causal effect, as the treatment effect can be attributed to the specific, well-defined treatment. Violation of the causal consistency assumption occurs when there are multiple versions of the treatment, and the treatment effect is not well-defined.\n\n\n\n\n\n\nTotal cholesterol level example\n\n\n\nIf some patients receive 5 mg of Rosuvastatin, while others receive 10 mg or 20 mg, it would be unclear which specific version of the treatment is responsible for the observed effect on total cholesterol levels. In such cases, the causal effect estimation may be biased or difficult to interpret.\n\n\n\n\n\n\n\n\nSolutions: Design vs Analysis\n\n\n\n\nTo ensure causal consistency, researchers should carefully design their study to control the treatment version and dosage, making sure that all treated individuals receive the same version of the treatment.\nIn cases where multiple treatment versions are present, researchers may need to modify their analysis to account for these different versions, such as by estimating separate causal effects for each dosage level or considering the treatment as a continuous variable (e.g., dose-response analysis)."
  },
  {
    "objectID": "assumptions.html#no-interference",
    "href": "assumptions.html#no-interference",
    "title": "3  Identifiability Conditions",
    "section": "3.4 No interference",
    "text": "3.4 No interference\nThe potential outcome for one individual is not affected by the treatment status of any other individual. This means that each individual’s treatment has no influence on the outcome of others, and the potential outcomes for a given individual are independent of the treatment assignments for other individuals.\n\n\n\n\n\n\nTotal cholesterol level example\n\n\n\nIf some patients in the treatment group are sharing their Rosuvastatin with control patients, the no interference assumption would be violated. This is because the treatment status of one individual is now affecting the potential outcomes of another individual, which goes against the no interference assumption. In such a scenario, the treatment effect estimation may be biased because the control group is not truly “untreated” and is receiving the treatment indirectly."
  },
  {
    "objectID": "assumptions.html#models-are-correctly-specified",
    "href": "assumptions.html#models-are-correctly-specified",
    "title": "3  Identifiability Conditions",
    "section": "3.5 Models are correctly specified",
    "text": "3.5 Models are correctly specified\nWe also assume that models used to estimate (1) propensity scores and (2) analyze outcomes are accurate representations of the underlying relationships between the variables involved. Model specification not only includes adding necessary covariates (which is also relavant for conditional exchangeability or no unmeasured confounding assumption), but also that they are added in the correct functional form.\n\n\n\n\n\n\nTotal cholesterol level example\n\n\n\nIf there is evidence that age and sex interact in determining the probability of being prescribed to Rosuvastatin, we should include the interaction term (age × sex) in the propensity score model along with the main effects.\nBy including the relevant interaction terms in the propensity score model, we can improve the model’s ability to accurately balance the covariates between the treatment and control groups, which in turn enhances the validity of the causal effect estimates.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nEnsuring that the identifiability assumptions hold is crucial for obtaining unbiased and meaningful causal effect estimates. When these assumptions are violated, the causal effect estimates may be biased or not identifiable at all.\nBy providing evidence or arguments supporting the plausibility of the identifiability assumptions in the study and addressing any potential violations (via sensitility analyses), we will increase the credibility and validity of the causal inference.\nMerely stating the assumptions without ensuring that they hold in our data can lead to biased or misleading results and undermine the validity of the study findings."
  },
  {
    "objectID": "extensions.html#gradient-boosted-models",
    "href": "extensions.html#gradient-boosted-models",
    "title": "4  Extensions",
    "section": "4.1 Gradient Boosted Models",
    "text": "4.1 Gradient Boosted Models\nGradient Boosted Models (GBM) are an ensemble technique used in machine learning. This is ensemble technique as combines multiple shallow decision trees (usually known to be weak learners) to create a more useful predictive model. It improves the overall model performance by iteratively adding these weak models that focus on correcting the errors of the previous models in the sequence. These models are very promising in a real-world data analysis scenario as they are can automatically incorporate nonlinear and interaction terms among the covariates.\nTraditional logistic regression models require manual specification of interaction terms, which can be challenging and time-consuming, especially if there are many potential interactions to consider. TWANG package in R implements GBMs to estimate propensity scores can help overcome this challenge. GBMs can automatically capture interaction effects and non-linear relationships among covariates without the need for manual specification. This ability to model interactions without explicit intervention from the analyst makes GBMs an appealing choice for propensity score estimation in complex datasets with potential interactions."
  },
  {
    "objectID": "extensions.html#propensity-score-weighting",
    "href": "extensions.html#propensity-score-weighting",
    "title": "4  Extensions",
    "section": "4.2 Propensity score weighting",
    "text": "4.2 Propensity score weighting\n\n\n\n\nflowchart LR\n  AA((propensity score weighting)) --> A[(Exposure modelling <br/>to estimate <br/>propensity scores)] \n  AA --> B[(Converting propensity scores  <br/>to  <br/>inverse probability weights,  <br/>or IPW)]\n  AA --> C[(Assess balance <br/>and summary of IPW  <br/>in the weighted data)]\n  AA --> D[(Outcome <br/>modelling in <br/>the IPW weighted data)]\n  style AA fill:#f9f,stroke:#333,stroke-width:4px\n\n\n\n\n\n\n\n\n\nAverage Treatment Effect (ATE)\n\\[\\begin{align*}\n    IPW_i &= \\begin{cases}\n        \\frac{1}{\\text{PS}_i} & \\text{if individual } i \\text{ is treated (} A_i = 1 \\text{)} \\\\\n        \\frac{1}{1 - \\text{PS}_i} & \\text{if individual } i \\text{ is not treated (} A_i = 0 \\text{)}\n    \\end{cases}\n\\end{align*}\\]\n\n\nAverage Treatment Effect on the Treated (ATT)\n\\[\\begin{align*}\n    IPW_i &= \\begin{cases}\n        1 & \\text{if individual } i \\text{ is treated (} A_i = 1 \\text{)} \\\\\n        \\frac{\\text{PS}_i}{1 - \\text{PS}_i} & \\text{if individual } i \\text{ is not treated (} A_i = 0 \\text{)}\n    \\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "extensions.html#categorical-exposure",
    "href": "extensions.html#categorical-exposure",
    "title": "4  Extensions",
    "section": "4.3 Categorical exposure",
    "text": "4.3 Categorical exposure\n\n4.3.1 Alcohol and Other Drug treatment data\n\nrequire(twang)\ndata(AOD)\nhead(AOD)\n\n\n\n  \n\n\ntable(AOD$treat)\n#> \n#> community   metcbt5       scy \n#>       200       200       200\n\nHere,\n\nOutcome variable: suf12: substance use frequency at 12 month follow-up\nexposure variable: treat: possible values\n\ncommunity: traditional programs (community)\nmetcbt5: evidence-based MET/CBT-5 treatment protocol\nscy: Strengthening Communities for Youth\n\nCovariates (pre-treatment):\n\nillact: illicit activities scale\ncrimjust: criminal justice involvement\nsubprob: substance use problem scale\nsubdep: substance use dependence scale\nwhite: non-Hispanic white youth\n\n\n\n\n4.3.2 Step 1: Estimation of Propensity scores\nWe can fit vector generalized linear models (VGLMs) to fit propensity score model with our 3 category exposure variable:\n\nrequire(VGAM)\npsFormula <- \"treat ~ illact + crimjust + subprob + subdep + white\"\nps.model <- vglm(psFormula,family=multinomial, data=AOD)\nAOD$ps <- data.frame(fitted(ps.model))\n\n\nrequire(ggplot2)\nggplot(AOD, aes(x=ps[,1], fill=factor(treat))) +\n  geom_density(alpha=0.5) +\n  scale_fill_discrete(name=\"Treatment Group\") +\n  labs(title=\"Density Plot of Propensity Scores by Treatment Group\",\n       x=\"Propensity Score\",\n       y=\"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n4.3.3 Step 1: Estimation of Propensity scores via GBM\n\nset.seed(1235)\nmnps.AOD.ATT <- mnps(treat ~ illact + crimjust + subprob + subdep + white, \n                 data = AOD, \n                 interaction.depth = 3,\n                 estimand = \"ATT\", \n                 treatATT = \"community\", # the treated\n                 verbose = FALSE, \n                 stop.method = \"es.mean\", \n                 n.trees = 1000)\n\n\n\n4.3.4 Step 2: IPW calculation for ATT\n\nAOD$w.ATT <- twang::get.weights(mnps.AOD.ATT, stop.method = \"es.mean\")\nsummary(AOD$w.ATT)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.1494  0.7265  1.0000  0.8896  1.0000  1.8823\nby(AOD$w.ATT, AOD$treat, summary)\n#> AOD$treat: community\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       1       1       1       1       1       1 \n#> ------------------------------------------------------------ \n#> AOD$treat: metcbt5\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.1494  0.4939  0.7204  0.7529  0.9460  1.8386 \n#> ------------------------------------------------------------ \n#> AOD$treat: scy\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.3548  0.7619  0.9630  0.9159  1.0263  1.8823\n\n\n\n4.3.5 Step 3: Balance assessment in weighted data\n\ntwang::bal.table(mnps.AOD.ATT, \n                 digits = 2, \n                 collapse.to = \"covariate\")[,c(\"max.std.eff.sz\",\n                                               \"stop.method\")]\n\n\n\n  \n\n\nplot(mnps.AOD.ATT, plots = 3)\n\n\n\n\n\n\n4.3.6 Step 4: Effect estimates from weighted outcome model\n\nrequire(survey)\ndesign.mnps.ATT <- svydesign(ids=~1, weights=~w.ATT, data=AOD)\nfit <- svyglm(suf12 ~ treat, design = design.mnps.ATT)\nrequire(Publish)\npublish(fit, intercept = FALSE)\n#>  Variable     Units Coefficient        CI.95   p-value \n#>     treat community         Ref                        \n#>             metcbt5        0.20 [-0.00;0.41]   0.05129 \n#>                 scy        0.08 [-0.11;0.27]   0.41503"
  },
  {
    "objectID": "extensions.html#continuous-exposure",
    "href": "extensions.html#continuous-exposure",
    "title": "4  Extensions",
    "section": "4.4 Continuous exposure",
    "text": "4.4 Continuous exposure\n\nrequire(twangContinuous)\nset.seed(1235)\ndata(dat)\nsummary(dat$tss_0) # treatment\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   0.000   0.000   0.000   2.101   4.000  13.000\n\n\n4.4.1 Step 1: Estimation of Propensity scores via GBM\n\ntest.mod <- ps.cont(tss_0 ~ sfs8p_0 + sati_0 + sp_sm_0 \n                    + recov_0 + subsgrps_n + treat,\n                    data=dat,\n                    n.trees = 500,\n                    shrinkage = 0.01,\n                    interaction.depth = 3,\n                    verbose = FALSE)\ntwangContinuous::bal.table(test.mod, digits = 3)\n\n\n\n  \n\n\n\n\n\n4.4.2 Step 2: IPW calculation for ATT\n\ndat$wts <- twangContinuous::get.weights(test.mod) \nsummary(dat$wts)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.04231 0.85715 0.90552 0.99339 1.05425 6.31356\nby(dat$wts, dat$tss_0, summary)\n#> dat$tss_0: 0\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.8552  0.8667  0.8956  0.9761  0.9388  6.3136 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 0.100623880265075\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   0.861   0.861   0.861   0.861   0.861   0.861 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 0.198553075924\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.9018  1.0325  1.1632  1.0852  1.1769  1.1907 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 0.881942079939801\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   1.146   1.146   1.146   1.146   1.146   1.146 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 1\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.9006  0.9063  0.9164  0.9831  0.9750  1.9217 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 1.15786092473523\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.9385  0.9385  0.9385  0.9385  0.9385  0.9385 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 2\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.9425  0.9451  0.9525  0.9652  0.9591  1.1788 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 2.31413199242876\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.9428  0.9489  0.9550  0.9550  0.9611  0.9672 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 2.32625317004906\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   1.019   1.019   1.019   1.019   1.019   1.019 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 2.99706024709897\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.9094  0.9094  0.9094  0.9094  0.9094  0.9094 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 3\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.9092  0.9333  0.9819  0.9876  1.0123  1.9738 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 3.45322259142172\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.9896  0.9896  0.9896  0.9896  0.9896  0.9896 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 4\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.8014  0.8661  1.0279  0.9941  1.0838  1.1252 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 5\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.6471  0.8081  1.0686  0.9874  1.1393  1.2394 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 6\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.4869  0.7553  1.1391  1.0271  1.2909  1.3806 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 7\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.3248  0.6921  1.0986  1.0161  1.2804  1.5554 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 8\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.2159  0.5560  0.7457  0.9648  1.4072  1.7720 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 9\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.1377  0.4927  1.1412  1.0919  1.5641  2.0416 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 10\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.0957  0.4447  1.1916  1.1865  1.9366  2.3789 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 11\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.07462 0.36352 0.88760 1.21486 1.98267 2.80320 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 12\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.04231 0.42725 0.76566 1.17997 2.14189 3.28406 \n#> ------------------------------------------------------------ \n#> dat$tss_0: 13\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> 0.04522 0.35156 0.40140 0.96447 0.98744 4.02582\n\n\n\n4.4.3 Step 3: Balance assessment in weighted data\n\nbal.table(test.mod, digits = 3)\n\n\n\n  \n\n\nplot(test.mod, plots=\"es\")\n\n\n\n\n\n\n4.4.4 Step 4: Effect estimates from weighted outcome model\n\ndesign.ps <- svydesign(ids=~1, weights=~wts, data=dat) \noutcome.model <- svyglm(sfs8p_3 ~ tss_0, design = design.ps, \n                        family = gaussian())\npublish(outcome.model, intercept = FALSE, digits = 4)\n#>  Variable Units Coefficient            CI.95  p-value \n#>     tss_0            0.0026 [-0.1197;0.1249]   0.9666"
  },
  {
    "objectID": "extensions.html#multilevel-modelling",
    "href": "extensions.html#multilevel-modelling",
    "title": "4  Extensions",
    "section": "4.5 Multilevel modelling",
    "text": "4.5 Multilevel modelling\nLi, Zaslavsky, and Landrum (2013) showed that “exploiting the multilevel structure in at least one stage can greatly reduce the bias”. They emphasize that propensity score methods offer a more robust alternative to regression adjustment, especially in complex multilevel observational data where correctly specifying the outcome model may be challenging.\n\n\n\n\nLi, Fan, Alan M Zaslavsky, and Mary Beth Landrum. 2013. “Propensity Score Weighting with Multilevel Data.” Statistics in Medicine 32 (19): 3373–87."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Li, Fan, Alan M Zaslavsky, and Mary Beth Landrum. 2013.\n“Propensity Score Weighting with Multilevel Data.”\nStatistics in Medicine 32 (19): 3373–87."
  }
]